{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_sentimental_analysis_naver_movie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOSbtPpLB7jlNnIKRzEODl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cypision/Alchemy-in-MLDL/blob/master/CNN_sentimental_analysis_naver_movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7p_eBRKz4mn"
      },
      "source": [
        "# 준비단계 - 필요 라이브러리 설치 등\n",
        "네이버 영화 리뷰 data는 별도로 제공받은 파일(출처공개 불가. 죄송 ㅜ.ㅜ)을 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZzSLSF8yvHA",
        "outputId": "ab595c21-4a1d-4579-97c2-1e5e2044f452"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7z2XFi9AlDp",
        "outputId": "36386311-9a67-4d22-98f1-c7b509f7fc10"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install jpype1==0.7.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 168kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: tweepy, beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n",
            "Collecting jpype1==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 12.7MB/s \n",
            "\u001b[?25hInstalling collected packages: jpype1\n",
            "  Found existing installation: JPype1 1.2.0\n",
            "    Uninstalling JPype1-1.2.0:\n",
            "      Successfully uninstalled JPype1-1.2.0\n",
            "Successfully installed jpype1-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URntkSHIAcXp"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe-jo3TEbbt"
      },
      "source": [
        "# from nltk.corpus import stopwords : 한국어는 직접 불용어를 설정해야만 한다."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmvCqxESz79H"
      },
      "source": [
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_train.json\") as f:\n",
        "  train = json.loads(f.read())\n",
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_val.json\") as f:\n",
        "  val = json.loads(f.read())\n",
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_test.json\") as f:\n",
        "  test = json.loads(f.read())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4qVKUhlA62-"
      },
      "source": [
        "## 형태소 분석기 불러오기\n",
        "from konlpy.tag import Komoran, Hannanum, Kkma, Okt ## using only Komoran object\n",
        "\n",
        "## 형태소분석 함수 만들기\n",
        "komoran = Komoran()\n",
        "def tokenize(word):\n",
        "  return komoran.morphs(word)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62DAvPv1DVpc",
        "outputId": "b833b292-3f0b-477c-b023-ce211180aa01"
      },
      "source": [
        "print(test[0])\n",
        "print(test[0][1])\n",
        "tokenize(test[0][1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1458790', '허우 샤오시엔 작품은 모두 만점!', '긍정']\n",
            "허우 샤오시엔 작품은 모두 만점!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['허', '우', '샤오', '시엔', '작품', '은', '모두', '만점', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG9VbOxgGlPA",
        "outputId": "3bd04c69-252f-44d4-91a3-a2e0fbef7b2d"
      },
      "source": [
        "## train,val,test 따로 떨어져 있으니 이를 한꺼번에 합친이후 단어사전을 만든다.\n",
        "print(len(train),len(val),len(test))\n",
        "total_setence = []\n",
        "\n",
        "total_setence.extend(train)\n",
        "total_setence.extend(val)\n",
        "total_setence.extend(test)\n",
        "\n",
        "print(len(total_setence))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 10000 10000\n",
            "70000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llQQJ7YVHUOx",
        "outputId": "64c1dcde-7784-4279-d8d7-e895b6bd5121"
      },
      "source": [
        "total_setence[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7570203', '장쯔이 그때나 지금이나 이뻤다', '긍정']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXAhklZ27n8s",
        "outputId": "1d8e68df-d5ea-4a32-f074-d7683a885f0c"
      },
      "source": [
        "## label 구성보기  \n",
        "label = []\n",
        "for lst in total_setence:\n",
        "  label.append(lst[2])\n",
        "\n",
        "label = set(label)\n",
        "print(label)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'부정', '긍정'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x8wJHxXu8HQj",
        "outputId": "9cb239fb-937e-42e7-8dcc-5a5b6d4ac55d"
      },
      "source": [
        "list(label)[0]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'부정'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-0V410r7-3h",
        "outputId": "ce63526f-fe1e-4d4e-b866-d0e1dc103dcd"
      },
      "source": [
        "label_map = {list(label)[0]:0,list(label)[1]:1}\n",
        "print(label_map)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'부정': 0, '긍정': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eduS6vaYA7If",
        "outputId": "bdc393bf-3557-4562-9279-dc58b6b34f9b"
      },
      "source": [
        "## train set 에서 단어사전 만들기  (불용어 처리 없이 간다.)\n",
        "import collections \n",
        "from tqdm import tqdm\n",
        "\n",
        "tot_tokens = 0\n",
        "char_counter = collections.Counter() # 카운터\n",
        "\n",
        "for dat in tqdm(total_setence):\n",
        "  sent = dat[1] ## real setence\n",
        "  tokenized_sent = tokenize(sent)\n",
        "  for cha in tokenized_sent:\n",
        "    char_counter[cha] += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [00:53<00:00, 1312.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-pfKz1EA7UO",
        "outputId": "8f22427c-ec17-45c9-9bb4-c90ced7bbd11"
      },
      "source": [
        "print(len(char_counter))\n",
        "token_dict = char_counter.most_common()\n",
        "print(type(token_dict))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38724\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajMZmoGtA7dp",
        "outputId": "8562def6-cd1e-44a8-a6bf-8d548c948327"
      },
      "source": [
        "token_dict[0:20]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 52649),\n",
              " ('이', 46168),\n",
              " ('하', 44135),\n",
              " ('ㄴ', 33430),\n",
              " ('는', 30228),\n",
              " ('다', 24603),\n",
              " ('영화', 24235),\n",
              " ('보', 21189),\n",
              " ('고', 20736),\n",
              " ('에', 15071),\n",
              " ('가', 14922),\n",
              " ('의', 14667),\n",
              " ('은', 14231),\n",
              " ('도', 14015),\n",
              " ('았', 13778),\n",
              " ('을', 13714),\n",
              " ('게', 12969),\n",
              " ('...', 11855),\n",
              " ('었', 11833),\n",
              " ('ㄹ', 11175)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNSwxOF8A7sF"
      },
      "source": [
        "##  불용어 처리 없이, index화 합니다. 이때, max_len 을 지정해서, 최종적으로 train,val,test 데이터 셋을 학습가능토록 형태를 만듭니다.  \n",
        "char_to_idx = {}\n",
        "idx_to_char = {}\n",
        "\n",
        "idx = 0\n",
        "for key,cnt in token_dict:\n",
        "  char_to_idx[key] = idx\n",
        "  idx += 1\n",
        "\n",
        "for key,value in char_to_idx.items():\n",
        "  idx_to_char[value] = key "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hLPp_ayA7x-",
        "outputId": "89874819-97ac-4c7a-d2a9-da96bdab33fa"
      },
      "source": [
        "print(token_dict[0])\n",
        "print(char_to_idx[token_dict[0][0]])\n",
        "print(idx_to_char[char_to_idx[token_dict[0][0]]])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('.', 52649)\n",
            "0\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBgYoVOJ3sFR"
      },
      "source": [
        "**단어사전이 완성되었습니다.**  \n",
        "상기과정에 [PAD],[UNK] 를 추가하여, 클래스로 만든것이 TextEncoder 입니다.  \n",
        "하기에서는 그 class를 불러서 만들겠습니다.  \n",
        "class가 하는 내용은 상기과정과 똑같지만, 깔끔하게 만들기 위해 class화 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAaaMist49ac"
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/CNS_NLP/utils.py\" \"/content\""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf-eV2_D4Ifc"
      },
      "source": [
        "from utils import TextEncoder"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abidRJ3n4Z9C"
      },
      "source": [
        "tuple_list = char_counter.most_common(len(char_counter))\n",
        "\n",
        "vocab_list = [\"[PAD]\", \"[UNK]\"]\n",
        "vocab_list.extend([t[0] for t in tuple_list])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WjnJst9m4qoW",
        "outputId": "3f3d69f5-6bdd-4317-d263-ce05dcb0aea0"
      },
      "source": [
        "print(len(vocab_list))\n",
        "vocab_list[2]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqwcDfSI5UQ9"
      },
      "source": [
        "text_encoder = TextEncoder(vocab_list)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuW9MW1W6NBf",
        "outputId": "1646b63c-ce7a-46a6-e0f7-87aa3af74511"
      },
      "source": [
        "tokenize(\"어떻게 토큰화되는지 확인해 봅시다.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['어떻', '게', '토큰', '화', '되', '는지', '확인', '하', '아', '보', 'ㅂ시다', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAzNsCu8A73l",
        "outputId": "342546bb-2385-4dc5-be39-d8acaf4103e7"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUU79gAu52RQ",
        "outputId": "ad55a9cd-fc2a-41c9-8010-265dd54e5262"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7570203', '장쯔이 그때나 지금이나 이뻤다', '긍정']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQVAI4lk8bn6",
        "outputId": "40aef8e0-972a-4203-fa21-e1a0f1b930d9"
      },
      "source": [
        "z = train[0][1]\n",
        "text_encoder.convert_tokens_to_ids(z)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[834, 1, 3, 1, 81, 87, 30, 1, 25, 758, 3, 30, 1, 3, 1, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbMYzyD7AERs",
        "outputId": "474b7e5a-d73a-493c-dbfc-54982d929256"
      },
      "source": [
        "label_map"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'긍정': 1, '부정': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQorkdQ652Yp"
      },
      "source": [
        "## text_encoder를 활용하여, 정수 index 구성으로 바꾸기\n",
        "train_input_lst = []\n",
        "val_input_lst = []\n",
        "test_input_lst = []\n",
        "\n",
        "train_label = []\n",
        "val_label = []\n",
        "test_label = []\n",
        "\n",
        "for data in train:\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(z)\n",
        "  train_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.\n",
        "  train_label.append(label_map[data[2]])\n",
        "\n",
        "for data in val:\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(z)\n",
        "  val_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.\n",
        "  val_label.append(label_map[data[2]])\n",
        "\n",
        "for data in test:\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(z)\n",
        "  test_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.  \n",
        "  test_label.append(label_map[data[2]])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33zdafVGAlZ5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63gG9ray9dHR"
      },
      "source": [
        "## 길이가 일정한 문장으로 만들기  \n",
        "## \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "\n",
        "max_seq_len = 150\n",
        "train_ids = pad_sequences(train_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "val_ids = pad_sequences(val_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "test_ids = pad_sequences(test_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ0_IkXrAuOo",
        "outputId": "9d4ea508-7818-43a2-d7c0-1b40687ce972"
      },
      "source": [
        "print(train_input_lst[11],\"\\n seuquence before: {}\".format(len(train_input_lst[11])))\n",
        "print(\"=\"*100)\n",
        "print(train_ids[11],\"\\n seuquence after: {}\".format(len(train_ids[11])))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[834, 1, 3, 1, 81, 87, 30, 1, 25, 758, 3, 30, 1, 3, 1, 7] \n",
            " seuquence before: 16\n",
            "====================================================================================================\n",
            "[834   1   3   1  81  87  30   1  25 758   3  30   1   3   1   7   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0] \n",
            " seuquence after: 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htvC5PaXDDoa",
        "outputId": "e096ac19-e506-47a4-d79d-af5e17ddb288"
      },
      "source": [
        "type(val_label)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M2ZFX3CAgVC"
      },
      "source": [
        "## 데이터 저장하기\n",
        "## train_input_lst,train_label,val_input_lst,val_label,test_input_lst,test_label\n",
        "## train_ids 를 저장하고 싶었으나, numpy의 경우, json 파일로 저장이 되지 않기 때문에 통일성을 위해, 그냥 pad sequence 이전것을 저장한다.  \n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_train_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(train_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_train_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(train_label, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_val_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(val_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_val_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(val_label, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_test_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(test_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_test_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(test_label, f,indent=\"\\t\")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5vNIjS_D3fX"
      },
      "source": [
        "**이후 과정은 별도 posting으로 운영한다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAJX3tuhD9rb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
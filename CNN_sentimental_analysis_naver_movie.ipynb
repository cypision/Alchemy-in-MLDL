{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_sentimental_analysis_naver_movie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5G0KzxIDridArRWM7cMqg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cypision/Alchemy-in-MLDL/blob/master/CNN_sentimental_analysis_naver_movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7p_eBRKz4mn"
      },
      "source": [
        "# 준비단계 - 필요 라이브러리 설치 등\n",
        "네이버 영화 리뷰 data는 별도로 제공받은 파일(출처공개 불가. 죄송 ㅜ.ㅜ)을 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZzSLSF8yvHA",
        "outputId": "95f9b06d-62a1-4978-b99d-79d0557dfe90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7z2XFi9AlDp",
        "outputId": "44e4732e-79ef-479b-b79b-e15b6f5e82af"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install jpype1==0.7.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 73.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, tweepy, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n",
            "Collecting jpype1==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 7.5MB/s \n",
            "\u001b[?25hInstalling collected packages: jpype1\n",
            "  Found existing installation: JPype1 1.2.0\n",
            "    Uninstalling JPype1-1.2.0:\n",
            "      Successfully uninstalled JPype1-1.2.0\n",
            "Successfully installed jpype1-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URntkSHIAcXp"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe-jo3TEbbt"
      },
      "source": [
        "# from nltk.corpus import stopwords : 한국어는 직접 불용어를 설정해야만 한다."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmvCqxESz79H"
      },
      "source": [
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_train.json\") as f:\n",
        "  train = json.loads(f.read())\n",
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_val.json\") as f:\n",
        "  val = json.loads(f.read())\n",
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_test.json\") as f:\n",
        "  test = json.loads(f.read())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4qVKUhlA62-"
      },
      "source": [
        "## 형태소 분석기 불러오기\n",
        "from konlpy.tag import Komoran, Hannanum, Kkma, Okt ## using only Komoran object\n",
        "\n",
        "## 형태소분석 함수 만들기\n",
        "komoran = Komoran()\n",
        "def tokenize(word):\n",
        "  return komoran.morphs(word)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62DAvPv1DVpc",
        "outputId": "1e21b1c4-a635-4925-9be1-56bb51063f9c"
      },
      "source": [
        "print(test[0])\n",
        "print(test[0][1])\n",
        "tokenize(test[0][1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1458790', '허우 샤오시엔 작품은 모두 만점!', '긍정']\n",
            "허우 샤오시엔 작품은 모두 만점!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['허', '우', '샤오', '시엔', '작품', '은', '모두', '만점', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG9VbOxgGlPA",
        "outputId": "d8df3636-6045-4e08-a0ac-c1684b380c58"
      },
      "source": [
        "## train,val,test 따로 떨어져 있으니 이를 한꺼번에 합친이후 단어사전을 만든다.\n",
        "print(len(train),len(val),len(test))\n",
        "total_setence = []\n",
        "\n",
        "total_setence.extend(train)\n",
        "total_setence.extend(val)\n",
        "total_setence.extend(test)\n",
        "\n",
        "print(len(total_setence))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 10000 10000\n",
            "70000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llQQJ7YVHUOx",
        "outputId": "f5788098-3732-4a80-9983-2543bdb3a22a"
      },
      "source": [
        "total_setence[11]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6950777', '역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*', '긍정']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXAhklZ27n8s",
        "outputId": "efa5c7b1-efd3-4f72-9ebd-48c15dfb01b1"
      },
      "source": [
        "## label 구성보기  \n",
        "label = []\n",
        "for lst in total_setence:\n",
        "  label.append(lst[2])\n",
        "\n",
        "label = set(label)\n",
        "print(label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'부정', '긍정'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x8wJHxXu8HQj",
        "outputId": "9875200e-8e33-4bdd-ea5f-6bdba51df92a"
      },
      "source": [
        "list(label)[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'부정'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-0V410r7-3h",
        "outputId": "9022417a-aedd-45ba-a4b2-847c4fc03dbc"
      },
      "source": [
        "label_map = {list(label)[0]:0,list(label)[1]:1}\n",
        "print(label_map)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'부정': 0, '긍정': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eduS6vaYA7If",
        "outputId": "29315115-efc8-4883-ab07-e68676890648"
      },
      "source": [
        "## train set 에서 단어사전 만들기  (불용어 처리 없이 간다.)\n",
        "import collections \n",
        "from tqdm import tqdm\n",
        "\n",
        "tot_tokens = 0\n",
        "komoran_counter = collections.Counter() # 카운터\n",
        "\n",
        "for dat in tqdm(total_setence):\n",
        "  sent = dat[1] ## real setence\n",
        "  tokenized_sent = tokenize(sent)\n",
        "  for cha in tokenized_sent:\n",
        "    komoran_counter[cha] += 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [00:53<00:00, 1315.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-pfKz1EA7UO",
        "outputId": "e2511675-2c3a-4895-d059-5b4c022d570f"
      },
      "source": [
        "print(len(komoran_counter))\n",
        "tokens = komoran_counter.most_common()\n",
        "print(type(tokens),type(tokens[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38724\n",
            "<class 'list'> <class 'tuple'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mn0v-mPU0tE"
      },
      "source": [
        "all_token = [to[0] for to in tokens]\n",
        "vocab_list = [\"[PAD]\",\"[UNK]\"]\n",
        "vocab_list.extend(all_token)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7FEzva-VRkZ",
        "outputId": "2a8587f4-5a73-44ad-adc7-32c6be06bd11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab_list[190]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'역시'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNSwxOF8A7sF"
      },
      "source": [
        "##  불용어 처리 없이, index화 합니다. 이때, max_len 을 지정해서, 최종적으로 train,val,test 데이터 셋을 학습가능토록 형태를 만듭니다.  \n",
        "char_to_idx = {}\n",
        "idx_to_char = {}\n",
        "\n",
        "for i, token in enumerate(vocab_list):\n",
        "  char_to_idx[token] = i\n",
        "\n",
        "for key,value in char_to_idx.items():\n",
        "  idx_to_char[value] = key "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hLPp_ayA7x-",
        "outputId": "77c97d34-2fe3-47ef-8fa5-2aa9b31b8ce1"
      },
      "source": [
        "print(vocab_list[190])\n",
        "print(char_to_idx[vocab_list[190]])\n",
        "print(idx_to_char[char_to_idx[vocab_list[190]]])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "역시\n",
            "190\n",
            "역시\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBgYoVOJ3sFR"
      },
      "source": [
        "**단어사전이 완성되었습니다.**  \n",
        "상기과정에 [PAD],[UNK] 를 추가하여, 클래스로 만든것이 TextEncoder 입니다.  \n",
        "하기에서는 그 class를 불러서 만들겠습니다.  \n",
        "class가 하는 내용은 상기과정과 똑같지만, 깔끔하게 만들기 위해 class화 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAaaMist49ac"
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/CNS_NLP/utils.py\" \"/content\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf-eV2_D4Ifc"
      },
      "source": [
        "from utils import TextEncoder"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjnJst9m4qoW",
        "outputId": "f856513a-b0be-4e9f-ad1e-4fb96c51c1d7"
      },
      "source": [
        "print(len(vocab_list))\n",
        "print(\"# Vocabs = {}\".format(len(vocab_list)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38726\n",
            "# Vocabs = 38726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zFwEZtYRm27q",
        "outputId": "8833b458-6f95-40e2-bac8-1428407693c0"
      },
      "source": [
        "vocab_list[190]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'역시'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqwcDfSI5UQ9"
      },
      "source": [
        "text_encoder = TextEncoder(vocab_list)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuW9MW1W6NBf"
      },
      "source": [
        "tokenize('역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQVAI4lk8bn6",
        "outputId": "fc909969-5e84-4f02-c3ad-d76c455aed50"
      },
      "source": [
        "print(train[11])\n",
        "z = train[11][1]\n",
        "rslt = text_encoder.convert_tokens_to_ids(z)\n",
        "print(rslt[0:10])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['6950777', '역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*', '긍정']\n",
            "[765, 89, 1, 440, 1, 1, 110, 1208, 58, 60]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntXSTAR9XByS",
        "outputId": "6c58cee4-9393-4473-bf36-b3c02c714707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "z"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WldEjZ8oW5R5"
      },
      "source": [
        "## 상기 코드에서 이상한점이 없는가?  z 는 문장인데? (필자가 틀린부분이라 크게 기록했다) ㅋ  \n",
        "이상한 점을 찾기 위해 아래 for 문을 보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbKhjMneXIWy",
        "outputId": "3781d0ca-0584-422c-d272-f8a0f0941096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in z :\n",
        "  print(i)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "역\n",
            "시\n",
            " \n",
            "미\n",
            "셸\n",
            " \n",
            "오\n",
            "슬\n",
            "로\n",
            "네\n",
            "요\n",
            "~\n",
            " \n",
            "이\n",
            " \n",
            "작\n",
            "품\n",
            "은\n",
            " \n",
            "꼭\n",
            " \n",
            "극\n",
            "장\n",
            "에\n",
            "서\n",
            " \n",
            "큰\n",
            " \n",
            "스\n",
            "크\n",
            "린\n",
            "으\n",
            "로\n",
            " \n",
            "봐\n",
            "야\n",
            "할\n",
            "듯\n",
            "해\n",
            "요\n",
            "~\n",
            " \n",
            "한\n",
            "번\n",
            " \n",
            "더\n",
            "봐\n",
            "야\n",
            "지\n",
            "~\n",
            "*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b79dhx0cXVkT"
      },
      "source": [
        "**그렇다. z 는 tokenize 되지 않는 그냥 문장이다. 이걸 적용하면, String Object로 인식하고 이 클래스는 기본적으로 한글자,한글자 ASCII 코드로 인식, 공백과 , 까지 인색하여 convert_to_idx 한다. 즉 형태소 형태로 분해되지 않은채 코딩되는 것이다. 이 부분을 주의해야 한다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAt7WzGbYSfz",
        "outputId": "3eb8d112-7605-48c7-b513-8b51f97da031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train[11]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6950777', '역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*', '긍정']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMPUd9VRY64T",
        "outputId": "7f822c0b-74ed-4152-ad64-9ff7966e0e7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenize(train[11][2])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['긍정']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ihSbfSX31s",
        "outputId": "68d82d6b-555c-4c7e-8eb9-ef7f049b88dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## train,val,test 를 정수형 index 벡터로 바꾸기 전에, 먼저 한덩어리로 되어 있는 setence를 tokenize해야 한다.\n",
        "tokenize_train = []\n",
        "tokenize_val = []\n",
        "tokenize_test = []\n",
        "\n",
        "for dat in tqdm(train):\n",
        "  sent = dat[1] ## real setence\n",
        "  tokenized_sentence = tokenize(sent)\n",
        "  rslt1, rslt2, rslt3 = dat[0],tokenized_sentence,dat[2]\n",
        "  tokenize_train.append([rslt1, rslt2, rslt3])\n",
        "\n",
        "for dat in tqdm(val):\n",
        "  sent = dat[1] ## real setence\n",
        "  tokenized_sentence = tokenize(sent)\n",
        "  rslt1, rslt2, rslt3 = dat[0],tokenized_sentence,dat[2]\n",
        "  tokenize_val.append([rslt1, rslt2, rslt3])\n",
        "\n",
        "for dat in tqdm(test):\n",
        "  sent = dat[1] ## real setence\n",
        "  tokenized_sentence = tokenize(sent)\n",
        "  rslt1, rslt2, rslt3 = dat[0],tokenized_sentence,dat[2]\n",
        "  tokenize_test.append([rslt1, rslt2, rslt3])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:36<00:00, 1383.33it/s]\n",
            "100%|██████████| 10000/10000 [00:07<00:00, 1387.07it/s]\n",
            "100%|██████████| 10000/10000 [00:07<00:00, 1363.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAzNsCu8A73l"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbMYzyD7AERs",
        "outputId": "70c6a680-eb69-4657-b1d7-8ffb056b1d4f"
      },
      "source": [
        "label_map"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'긍정': 1, '부정': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQorkdQ652Yp"
      },
      "source": [
        "## text_encoder를 활용하여, 정수 index 구성으로 바꾸기\n",
        "train_input_lst = []\n",
        "val_input_lst = []\n",
        "test_input_lst = []\n",
        "\n",
        "train_label = []\n",
        "val_label = []\n",
        "test_label = []\n",
        "\n",
        "for data in tokenize_train :\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(setence)\n",
        "  train_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.\n",
        "  train_label.append(label_map[data[2]])\n",
        "\n",
        "for data in tokenize_val :\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(setence)\n",
        "  val_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.\n",
        "  val_label.append(label_map[data[2]])\n",
        "\n",
        "for data in tokenize_test :\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(setence)\n",
        "  test_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.  \n",
        "  test_label.append(label_map[data[2]])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FhuvcmnoOOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41cdb950-2725-4cc6-8947-bd195663d1bd"
      },
      "source": [
        "setence00 = train[11][1]\n",
        "print(setence00)\n",
        "setence01 = tokenize_train[11][1]\n",
        "print(setence01)\n",
        "\n",
        "setence02 = text_encoder.convert_tokens_to_ids(setence01)\n",
        "print(setence02)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*\n",
            "['역시', '미셸', '오슬로', '네', '요', '~', '이', '작품', '은', '꼭', '극장', '에서', '크', 'ㄴ', '스크린', '으로', '보', '아야', '하', 'ㄹ', '듯', '하', '어요', '~', '한', '번', '더', '보', '아야지', '~', '*']\n",
            "[190, 8891, 10379, 60, 135, 33, 3, 128, 14, 220, 352, 57, 280, 5, 1708, 45, 9, 162, 4, 21, 114, 4, 53, 33, 94, 134, 96, 9, 683, 33, 632]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33zdafVGAlZ5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63gG9ray9dHR"
      },
      "source": [
        "## 길이가 일정한 문장으로 만들기  \n",
        "## \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "\n",
        "# input_ids = pad_sequences(input_ids,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "\n",
        "max_seq_len = 150\n",
        "train_ids = pad_sequences(train_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "val_ids = pad_sequences(val_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "test_ids = pad_sequences(test_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ0_IkXrAuOo",
        "outputId": "51c17a89-4213-461c-b5b5-51ddfeebe0db"
      },
      "source": [
        "print(train[11])\n",
        "print(\"=\"*500)\n",
        "print(train_input_lst[11],\"\\n seuquence before: {}\".format(len(train_input_lst[11])))\n",
        "print(\"=\"*500)\n",
        "print(train_ids[11],\"\\n seuquence after: {}\".format(len(train_ids[11])))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['6950777', '역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*', '긍정']\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "[190, 8891, 10379, 60, 135, 33, 3, 128, 14, 220, 352, 57, 280, 5, 1708, 45, 9, 162, 4, 21, 114, 4, 53, 33, 94, 134, 96, 9, 683, 33, 632] \n",
            " seuquence before: 31\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "[  190  8891 10379    60   135    33     3   128    14   220   352    57\n",
            "   280     5  1708    45     9   162     4    21   114     4    53    33\n",
            "    94   134    96     9   683    33   632     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0] \n",
            " seuquence after: 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ut3uRxMa1cO",
        "outputId": "d9e7ad8d-037b-40de-a107-5a511de461c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_label[11]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htvC5PaXDDoa",
        "outputId": "9ad38ed0-cb14-4994-ba98-f501e05f5d60"
      },
      "source": [
        "type(val_label)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M2ZFX3CAgVC"
      },
      "source": [
        "## 데이터 저장하기\n",
        "## train_input_lst,train_label,val_input_lst,val_label,test_input_lst,test_label\n",
        "## train_ids 를 저장하고 싶었으나, numpy의 경우, json 파일로 저장이 되지 않기 때문에 통일성을 위해, 그냥 pad sequence 이전것을 저장한다.  \n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_train_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(train_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_train_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(train_label, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_val_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(val_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_val_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(val_label, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_test_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(test_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_test_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(test_label, f,indent=\"\\t\")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5vNIjS_D3fX"
      },
      "source": [
        "**이후 과정은 별도 posting으로 운영한다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAJX3tuhD9rb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title:  \"CatBoost Titanic\"  \n",
    "excerpt: \"CatBoost, Ensemble, Gradient Descent\"  \n",
    "\n",
    "categories:  \n",
    "  - Machine-Learning  \n",
    "  \n",
    "tags:  \n",
    "  - Stacking  \n",
    "  - Ensemble  \n",
    "  - Medium  \n",
    "last_modified_at: 2020-06-13T15:00:00-05:00\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference  \n",
    "* [Medium Daniel Chepenko](https://towardsdatascience.com/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14) \n",
    "\n",
    "* [CatBoost 공식문서](https://catboost.ai/docs/)  \n",
    "\n",
    "* \"https://catboost.ai/docs/\"  \n",
    "Catboost tutorial은 상기 주소를 참조했습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('../dataset/titanic_train.csv')\n",
    "test = pd.read_csv('../dataset/titanic_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set() # setting seaborn default for plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## posting 목적상, EDA 부분은 생략합니다. 원본을 참조하시면, 훨씬 훌륭하게 잘 정리되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train, test] # combining train and test dataset\n",
    "for dataset in train_test_data:\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title map\n",
    "Mr : mr    \n",
    "Miss : miss  \n",
    "Mrs: mrs  \n",
    "Others: others  \n",
    "단순한 categorical 하게 컬럼값을 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_title(x):\n",
    "    if x == 'Mr':\n",
    "        rslt = 'mr'\n",
    "    elif x =='Miss':\n",
    "        rslt = 'miss'\n",
    "    elif x == 'Mrs':\n",
    "        rslt = 'mrs'\n",
    "    else :\n",
    "        rslt = 'others'\n",
    "    return rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train.Title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Title.map(lambda x : apply_title(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset['New_Title'] = dataset.apply(lambda x : apply_title(x['Title']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unnecessary feature from dataset\n",
    "train.drop('Name', axis=1, inplace=True)\n",
    "test.drop('Name', axis=1, inplace=True)\n",
    "train.drop('Title', axis=1, inplace=True)\n",
    "test.drop('Title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Null 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing age with median age for each title (Mr, Mrs, Miss, Others)\n",
    "train[\"Age\"].fillna(train.groupby(\"New_Title\")[\"Age\"].transform(\"median\"), inplace=True)\n",
    "test[\"Age\"].fillna(test.groupby(\"New_Title\")[\"Age\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head(30)\n",
    "print(train.groupby(\"New_Title\")[\"Age\"].agg(\"median\"))\n",
    "train.groupby(\"New_Title\")[\"Age\"].transform(\"median\")[0:10] ## provide median value associated with Title-Age per each line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more than 50% of 1st class are from S embark  \n",
    "more than 50% of 2nd class are from S embark  \n",
    "more than 50% of 3rd class are from S embark\n",
    "\n",
    "**fill out missing embark with S embark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,\n",
    "    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,\n",
    "    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,\n",
    "    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing Fare with median fare for each Pclass\n",
    "train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\n",
    "test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].fillna('U',inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n",
    "test[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop = ['Ticket', 'SibSp', 'Parch']\n",
    "train = train.drop(features_drop, axis=1)\n",
    "test = test.drop(features_drop, axis=1)\n",
    "train = train.drop(['PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare feature looks having correlation with Surviced. so Check the outlier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Fare.describe().apply(lambda x : \"{:.4f}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourlier_index(df,column,p):\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    iqr = q3 - q1\n",
    "    max_limit_val = q3+(iqr*p)\n",
    "    min_limit_val = 0 if q1-(iqr*p) < 0 else q1-(iqr*p)\n",
    "    a = (min_limit_val,max_limit_val)\n",
    "    print(\"min_limit_val {}\".format(a[0]),\"\\t\",\"max_limit_val {}\".format(a[1]))\n",
    "    ix = df.loc[df.Fare < a[0]].index | df.loc[df.Fare > a[1]].index\n",
    "    left_ix = set(df.index)-set(ix)\n",
    "    return left_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(by='Survived').Fare.describe().stack().apply(lambda x : \"{:.4f}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "survived = 1,0 에 따라 아웃라이어가 다를 수 있으니, 다르게 접근하여 아웃라이어를 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when you want to find outlier and remove outlier value, run this code~\n",
    "\n",
    "left_survive_ix = ourlier_index(train[train.loc[:,\"Survived\"]==1],\"Fare\",2.5)\n",
    "left_die_ix = ourlier_index(train[train.loc[:,\"Survived\"]==0],\"Fare\",2.5)\n",
    "\n",
    "print(len(left_survive_ix),len(left_die_ix))\n",
    "left_survive_ix.intersection(left_die_ix) ## no dup index\n",
    "\n",
    "train_01 = pd.concat([train.iloc[list(left_survive_ix)],train.iloc[list(left_die_ix)]],axis=0)\n",
    "train_01.reset_index(drop=True,inplace=True)\n",
    "print(train_01.shape)\n",
    "train_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(\"---\"*20)\n",
    "print(test.shape)\n",
    "print(\"---\"*20)\n",
    "print(train.isnull().sum())\n",
    "print(\"---\"*20)\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data 전처리 끝난 data 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../dataset/titanic_train.csv')\n",
    "# test = pd.read_csv('../dataset/titanic_test.csv')\n",
    "\n",
    "train.to_csv(\"../dataset/titanic_processed_train.csv\",index=False,encoding=\"UTF8\")\n",
    "test.to_csv(\"../dataset/titanic_processed_test.csv\",index=False,encoding=\"UTF8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/titanic_processed_train.csv')\n",
    "test = pd.read_csv('../dataset/titanic_processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.drop('Survived', axis=1)\n",
    "target = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.drop(\"PassengerId\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape,target.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numeric, categorical column 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train_data.columns[train_data.dtypes == 'object'].to_list()\n",
    "num_cols = train_data.columns[train_data.dtypes != 'object'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns)\n",
    "len(cat_cols)+len(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder,StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from imblearn.ensemble import BalancedRandomForestclassifier\n",
    "# import xgboost as sgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(train_data[num_cols])\n",
    "test_data_scaled = scaler.transform(test_data[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[num_cols] = x_train_scaled\n",
    "test_data[num_cols] = test_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Catboosting Modeling - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "print(catboost.__version__)\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train_data, target, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train,y_train, cat_features=cat_cols)\n",
    "eval_pool = Pool(X_validation , y_validation , cat_features=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'iterations':200,\n",
    "          'random_seed':63,\n",
    "          'learning_rate':0.02,\n",
    "          'loss_function':'Logloss', ## 사실 Default 값은 Logloss 이다. 만약 CatBoostRegressor 였으면, RMSE 이다.\n",
    "          'custom_metric':['Logloss','AUC'],##, '\n",
    "          'early_stopping_rounds':20,\n",
    "          'use_best_model': True,\n",
    "          'task_type':\"GPU\",\n",
    "          'bagging_temperature':1,\n",
    "          'verbose':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## model : use_best_model = false 인 모델\n",
    "model = CatBoostClassifier(**params)\n",
    "model.fit(train_pool, eval_set=eval_pool,plot=True) ## ,save_snapshot=True\n",
    "\n",
    "print('Simple model validation accuracy: {:.4}'.format(accuracy_score(y_validation, model.predict(X_validation))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**특이사항**  \n",
    "1) catboost lib 에서, 제공하는 Pool() 을 이용 편하게 묶어서 다닐 수 있다.  \n",
    "2) 모델에게 반드시 ```cat_features=cat_features``` 을 명시해야 한다. (fit method 에서도 가능하고, 상기 예제에서는 Pool()에서 선언함)  \n",
    "3) plot=True 란 명령어 하나로, 쉽게 시각화 가능하다.  \n",
    "4) scikit-learn 의 여타 library와 마찬가지로, dict_typing 형태의 상속함수를 사용하기 때문에, fit,predict,predict_proba() 등이 가능하다.\n",
    "5) parmeter 에 early_stopping_rounds, od_type , od_pval 같은 Overfitting detector 를 쉽게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Catboosting Modeling - CV 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({'early_stopping_rounds':None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = cv(\n",
    "    params = params,\n",
    "#     pool = Pool(X, label=y, cat_features=cat_features), ## fit  method 구문의 역할을 한다.\n",
    "    pool = train_pool, ## fit  method 구문의 역할을 한다.\n",
    "    fold_count=3,\n",
    "    shuffle=True,\n",
    "    partition_random_seed=0,\n",
    "    plot=True,\n",
    "    stratified=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 요약된 결과보기\n",
    "cv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_value = np.min(cv_data['test-Logloss-mean'])\n",
    "best_iter = np.argmin(cv_data['test-Logloss-mean'])\n",
    "\n",
    "print('Best validation Logloss score, not stratified: {:.4f}±{:.4f} on step {}'.format(\n",
    "    best_value,cv_data['test-Logloss-std'][best_iter], best_iter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(cv_data['test-Logloss-mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV 결과를 통해 평균적으로 fitted 된 모델의 성능을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Catboosting Modeling - Hyper parameter 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter 튜닝은 별도의 library를 활용해서, 하는데 error 부분이 있기에 여기선 생략한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool as pool\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 200,\n",
       " 'random_seed': 63,\n",
       " 'learning_rate': 0.02,\n",
       " 'loss_function': 'Logloss',\n",
       " 'custom_metric': ['Logloss', 'AUC'],\n",
       " 'early_stopping_rounds': None,\n",
       " 'use_best_model': True,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of random sampled hyperparameters\n",
    "N_HYPEROPT_PROBES = 15\n",
    "\n",
    "# the sampling aplgorithm \n",
    "HYPEROPT_ALGO = tpe.suggest \n",
    "\n",
    "def get_catboost_params(space):\n",
    "    tunning_params = dict()\n",
    "    tunning_params['learning_rate'] = space['learning_rate']\n",
    "    tunning_params['depth'] = int(space['depth'])\n",
    "    tunning_params['l2_leaf_reg'] = space['l2_leaf_reg']\n",
    "    tunning_params['loss_function'] = 'Logloss'\n",
    "#     tunning_params['one_hot_max_size'] = space['one_hot_max_size']\n",
    "    return tunning_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_call_count = 0\n",
    "cur_best_loss = np.inf\n",
    "log_writer = open( 'catboost-hyperopt-log.txt', 'w' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    global obj_call_count, cur_best_loss\n",
    "\n",
    "    obj_call_count += 1\n",
    "\n",
    "    print('\\nCatBoost objective call #{} cur_best_loss={:7.5f}'.format(obj_call_count,cur_best_loss) )\n",
    "\n",
    "    params = get_catboost_params(space)\n",
    "\n",
    "#     sorted_params = sorted(space.iteritems(), key=lambda z: z[0])\n",
    "#     params_str = str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params])\n",
    "#     print('Params: {}'.format(params_str) )\n",
    "    \n",
    "    model = CatBoostClassifier(iterations=2000,\n",
    "                               learning_rate=params['learning_rate'], \n",
    "                               depth =int(params['depth']), \n",
    "                               task_type = \"GPU\",\n",
    "                               eval_metric = \"AUC\",\n",
    "                               l2_leaf_reg=params['l2_leaf_reg'],\n",
    "                               bagging_temperature=1,\n",
    "                               use_best_model=True)\n",
    "\n",
    "    model.fit(train_pool, eval_set=eval_pool, silent=True)\n",
    "    #y_pred = model.predict(df_test_.drop('loss', axis=1))\n",
    "    val_loss = model.best_score_['validation']['Logloss']\n",
    "    \n",
    "    if val_loss<cur_best_loss:\n",
    "        cur_best_loss = val_loss\n",
    "\n",
    "    return{'loss':cur_best_loss, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "space ={\n",
    "        'depth': hp.quniform(\"depth\", 4, 12, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -3.0, -0.7),\n",
    "        'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10) \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function hyperopt.tpe.suggest(new_ids, domain, trials, seed, prior_weight=1.0, n_startup_jobs=20, n_EI_candidates=24, gamma=0.25, verbose=True)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HYPEROPT_ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \n",
      "CatBoost objective call #3 cur_best_loss=    inf\n",
      "                                                                                                                       \n",
      "CatBoost objective call #4 cur_best_loss=0.39896\n",
      "                                                                                                                       \n",
      "CatBoost objective call #5 cur_best_loss=0.39896\n",
      "                                                                                                                       \n",
      "CatBoost objective call #6 cur_best_loss=0.39896\n",
      "                                                                                                                       \n",
      "CatBoost objective call #7 cur_best_loss=0.39896\n",
      " 27%|█████████████                                    | 4/15 [06:09<16:31, 90.16s/trial, best loss: 0.3989643387730346]"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = hyperopt.fmin(fn=objective,space=space,algo=HYPEROPT_ALGO,max_evals=N_HYPEROPT_PROBES,trials=trials)\n",
    "\n",
    "print('-'*50)\n",
    "print('The best params:')\n",
    "print( best )\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 제출 결과 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_data)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": prediction\n",
    "    })\n",
    "\n",
    "submission.to_csv('../dataset/submission.csv', index=False) ## 0.79425 달성!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../dataset/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.79425 로 Stacking과 동일한 결과를 얻었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

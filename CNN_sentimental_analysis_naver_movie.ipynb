{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_sentimental_analysis_naver_movie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMijqX3lUvi/B9ql8UfQOTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cypision/Alchemy-in-MLDL/blob/master/CNN_sentimental_analysis_naver_movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7p_eBRKz4mn"
      },
      "source": [
        "# 준비단계 - 필요 라이브러리 설치 등\n",
        "네이버 영화 리뷰 data는 별도로 제공받은 파일(출처공개 불가. 죄송 ㅜ.ㅜ)을 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZzSLSF8yvHA",
        "outputId": "88ebb85d-9244-49b4-9245-360c19207bcf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7z2XFi9AlDp",
        "outputId": "b0a930a2-f5c7-462c-ef8b-a656f801cd4c"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install jpype1==0.7.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 42.2MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.7MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, beautifulsoup4, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n",
            "Collecting jpype1==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 10.8MB/s \n",
            "\u001b[?25hInstalling collected packages: jpype1\n",
            "  Found existing installation: JPype1 1.2.0\n",
            "    Uninstalling JPype1-1.2.0:\n",
            "      Successfully uninstalled JPype1-1.2.0\n",
            "Successfully installed jpype1-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URntkSHIAcXp"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe-jo3TEbbt"
      },
      "source": [
        "# from nltk.corpus import stopwords : 한국어는 직접 불용어를 설정해야만 한다."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmvCqxESz79H"
      },
      "source": [
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_train.json\") as f:\n",
        "  train = json.loads(f.read())\n",
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_val.json\") as f:\n",
        "  val = json.loads(f.read())\n",
        "with open(\"/content/gdrive/My Drive/CNS_NLP/Sentiment_test.json\") as f:\n",
        "  test = json.loads(f.read())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4qVKUhlA62-"
      },
      "source": [
        "## 형태소 분석기 불러오기\n",
        "from konlpy.tag import Komoran, Hannanum, Kkma, Okt ## using only Komoran object\n",
        "\n",
        "## 형태소분석 함수 만들기\n",
        "komoran = Komoran()\n",
        "def tokenize(word):\n",
        "  return komoran.morphs(word)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62DAvPv1DVpc",
        "outputId": "57ff1ffd-1638-4d2f-feee-1cf84e32b361"
      },
      "source": [
        "print(test[0])\n",
        "print(test[0][1])\n",
        "tokenize(test[0][1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1458790', '허우 샤오시엔 작품은 모두 만점!', '긍정']\n",
            "허우 샤오시엔 작품은 모두 만점!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['허', '우', '샤오', '시엔', '작품', '은', '모두', '만점', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG9VbOxgGlPA",
        "outputId": "0b4cda4b-88d9-4713-ee6b-e3e6a3df52ed"
      },
      "source": [
        "## train,val,test 따로 떨어져 있으니 이를 한꺼번에 합친이후 단어사전을 만든다.\n",
        "print(len(train),len(val),len(test))\n",
        "total_setence = []\n",
        "\n",
        "total_setence.extend(train)\n",
        "total_setence.extend(val)\n",
        "total_setence.extend(test)\n",
        "\n",
        "print(len(total_setence))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 10000 10000\n",
            "70000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llQQJ7YVHUOx",
        "outputId": "51ba7bfd-01a1-4180-eef1-be0bfe01c731"
      },
      "source": [
        "total_setence[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7570203', '장쯔이 그때나 지금이나 이뻤다', '긍정']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXAhklZ27n8s",
        "outputId": "5c146acc-ad70-4887-f5fa-f5134c62498d"
      },
      "source": [
        "## label 구성보기  \n",
        "label = []\n",
        "for lst in total_setence:\n",
        "  label.append(lst[2])\n",
        "\n",
        "label = set(label)\n",
        "print(label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'부정', '긍정'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x8wJHxXu8HQj",
        "outputId": "cbd77f63-4d1b-414d-aeb0-aeafa3b9667e"
      },
      "source": [
        "list(label)[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'부정'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-0V410r7-3h",
        "outputId": "22d45069-82bd-409b-ae25-245252e2c620"
      },
      "source": [
        "label_map = {list(label)[0]:0,list(label)[1]:1}\n",
        "print(label_map)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'부정': 0, '긍정': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eduS6vaYA7If",
        "outputId": "0d4161e6-88cc-4959-9e64-5d3efc03a1d3"
      },
      "source": [
        "## train set 에서 단어사전 만들기  (불용어 처리 없이 간다.)\n",
        "import collections \n",
        "from tqdm import tqdm\n",
        "\n",
        "tot_tokens = 0\n",
        "char_counter = collections.Counter() # 카운터\n",
        "\n",
        "for dat in tqdm(total_setence):\n",
        "  sent = dat[1] ## real setence\n",
        "  tokenized_sent = tokenize(sent)\n",
        "  for cha in tokenized_sent:\n",
        "    char_counter[cha] += 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [01:08<00:00, 1021.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-pfKz1EA7UO",
        "outputId": "ed4b6efa-9cfd-4682-da49-95d2f2fb38bb"
      },
      "source": [
        "print(len(char_counter))\n",
        "token_dict = char_counter.most_common()\n",
        "print(type(token_dict))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38724\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajMZmoGtA7dp",
        "outputId": "0d29133d-9073-431a-e940-51db6eb2c45b"
      },
      "source": [
        "token_dict[0:20]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 52649),\n",
              " ('이', 46168),\n",
              " ('하', 44135),\n",
              " ('ㄴ', 33430),\n",
              " ('는', 30228),\n",
              " ('다', 24603),\n",
              " ('영화', 24235),\n",
              " ('보', 21189),\n",
              " ('고', 20736),\n",
              " ('에', 15071),\n",
              " ('가', 14922),\n",
              " ('의', 14667),\n",
              " ('은', 14231),\n",
              " ('도', 14015),\n",
              " ('았', 13778),\n",
              " ('을', 13714),\n",
              " ('게', 12969),\n",
              " ('...', 11855),\n",
              " ('었', 11833),\n",
              " ('ㄹ', 11175)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNSwxOF8A7sF"
      },
      "source": [
        "##  불용어 처리 없이, index화 합니다. 이때, max_len 을 지정해서, 최종적으로 train,val,test 데이터 셋을 학습가능토록 형태를 만듭니다.  \n",
        "char_to_idx = {}\n",
        "idx_to_char = {}\n",
        "\n",
        "idx = 0\n",
        "for key,cnt in token_dict:\n",
        "  char_to_idx[key] = idx\n",
        "  idx += 1\n",
        "\n",
        "for key,value in char_to_idx.items():\n",
        "  idx_to_char[value] = key "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hLPp_ayA7x-",
        "outputId": "b21a3350-5259-460d-c94d-38f328f6490a"
      },
      "source": [
        "print(token_dict[0])\n",
        "print(char_to_idx[token_dict[0][0]])\n",
        "print(idx_to_char[char_to_idx[token_dict[0][0]]])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('.', 52649)\n",
            "0\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBgYoVOJ3sFR"
      },
      "source": [
        "**단어사전이 완성되었습니다.**  \n",
        "상기과정에 [PAD],[UNK] 를 추가하여, 클래스로 만든것이 TextEncoder 입니다.  \n",
        "하기에서는 그 class를 불러서 만들겠습니다.  \n",
        "class가 하는 내용은 상기과정과 똑같지만, 깔끔하게 만들기 위해 class화 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAaaMist49ac"
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/CNS_NLP/utils.py\" \"/content\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf-eV2_D4Ifc"
      },
      "source": [
        "from utils import TextEncoder"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abidRJ3n4Z9C"
      },
      "source": [
        "tuple_list = char_counter.most_common(len(char_counter))\n",
        "\n",
        "vocab_list = [\"[PAD]\", \"[UNK]\"]\n",
        "vocab_list.extend([t[0] for t in tuple_list])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjnJst9m4qoW",
        "outputId": "7718c4f4-b787-4693-a99f-6ea9feff13f7"
      },
      "source": [
        "print(len(vocab_list))\n",
        "\n",
        "print(\"# Vocabs = {}\".format(len(vocab_list)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38726\n",
            "# Vocabs = 38726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zFwEZtYRm27q",
        "outputId": "2bdfb6e3-776b-4418-d6a0-3ad8671aac73"
      },
      "source": [
        "vocab_list[89]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'시'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqwcDfSI5UQ9"
      },
      "source": [
        "text_encoder = TextEncoder(vocab_list)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuW9MW1W6NBf"
      },
      "source": [
        "tokenize('역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAzNsCu8A73l"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQVAI4lk8bn6",
        "outputId": "321b639c-ea07-42fd-8ddc-b2ba0ba8a02a"
      },
      "source": [
        "print(train[11])\n",
        "z = train[11][1]\n",
        "rslt = text_encoder.convert_tokens_to_ids(z)\n",
        "print(rslt[0:10])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['6950777', '역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*', '긍정']\n",
            "[765, 89, 1, 440, 1, 1, 110, 1208, 58, 60]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2MJfUDCqKBH",
        "outputId": "62fbe1cf-85a5-4de5-e5a2-000fc31f97cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab_list[765]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'역'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbMYzyD7AERs",
        "outputId": "7a50d9f7-81ac-42b0-ec7e-1a4241a14dad"
      },
      "source": [
        "label_map"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'긍정': 1, '부정': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQorkdQ652Yp"
      },
      "source": [
        "## text_encoder를 활용하여, 정수 index 구성으로 바꾸기\n",
        "train_input_lst = []\n",
        "val_input_lst = []\n",
        "test_input_lst = []\n",
        "\n",
        "train_label = []\n",
        "val_label = []\n",
        "test_label = []\n",
        "\n",
        "for data in train:\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(setence)\n",
        "  train_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.\n",
        "  train_label.append(label_map[data[2]])\n",
        "\n",
        "for data in val:\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(setence)\n",
        "  val_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.\n",
        "  val_label.append(label_map[data[2]])\n",
        "\n",
        "for data in test:\n",
        "  setence = data[1]\n",
        "  tokenized_idx_lst = text_encoder.convert_tokens_to_ids(setence)\n",
        "  test_input_lst.append(tokenized_idx_lst) ## 2중 list 형태로 데이터를 만든다.  \n",
        "  test_label.append(label_map[data[2]])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FhuvcmnoOOG",
        "outputId": "01339774-f2b6-480d-e47b-397c4bde988d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "setence = train[11][1]\n",
        "print(setence)\n",
        "tokenized_idx_lst = text_encoder.convert_tokens_to_ids(setence)\n",
        "print(tokenized_idx_lst)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*\n",
            "[765, 89, 1, 440, 1, 1, 110, 1208, 58, 60, 135, 33, 1, 3, 1, 214, 1923, 14, 1, 220, 1, 501, 834, 11, 198, 1, 1, 1, 27640, 280, 2115, 651, 58, 1, 1, 140, 2139, 114, 668, 135, 33, 1, 94, 134, 1, 96, 1, 140, 25, 33, 632]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33zdafVGAlZ5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63gG9ray9dHR"
      },
      "source": [
        "## 길이가 일정한 문장으로 만들기  \n",
        "## \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "\n",
        "# input_ids = pad_sequences(input_ids,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "\n",
        "max_seq_len = 150\n",
        "train_ids = pad_sequences(train_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "val_ids = pad_sequences(val_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")\n",
        "test_ids = pad_sequences(test_input_lst,maxlen=max_seq_len,padding=\"post\",truncating=\"pre\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ0_IkXrAuOo",
        "outputId": "3aa6e2ff-e97f-489b-d33c-5349440a7fc5"
      },
      "source": [
        "print(train[11])\n",
        "print(\"=\"*500)\n",
        "print(train_input_lst[11],\"\\n seuquence before: {}\".format(len(train_input_lst[11])))\n",
        "print(\"=\"*500)\n",
        "print(train_ids[11],\"\\n seuquence after: {}\".format(len(train_ids[11])))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['6950777', '역시 미셸 오슬로네요~ 이 작품은 꼭 극장에서 큰 스크린으로 봐야할듯해요~ 한번 더봐야지~*', '긍정']\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "[765, 89, 1, 440, 1, 1, 110, 1208, 58, 60, 135, 33, 1, 3, 1, 214, 1923, 14, 1, 220, 1, 501, 834, 11, 198, 1, 1, 1, 27640, 280, 2115, 651, 58, 1, 1, 140, 2139, 114, 668, 135, 33, 1, 94, 134, 1, 96, 1, 140, 25, 33, 632] \n",
            " seuquence before: 51\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "[  765    89     1   440     1     1   110  1208    58    60   135    33\n",
            "     1     3     1   214  1923    14     1   220     1   501   834    11\n",
            "   198     1     1     1 27640   280  2115   651    58     1     1   140\n",
            "  2139   114   668   135    33     1    94   134     1    96     1   140\n",
            "    25    33   632     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0] \n",
            " seuquence after: 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htvC5PaXDDoa",
        "outputId": "e096ac19-e506-47a4-d79d-af5e17ddb288"
      },
      "source": [
        "type(val_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M2ZFX3CAgVC"
      },
      "source": [
        "## 데이터 저장하기\n",
        "## train_input_lst,train_label,val_input_lst,val_label,test_input_lst,test_label\n",
        "## train_ids 를 저장하고 싶었으나, numpy의 경우, json 파일로 저장이 되지 않기 때문에 통일성을 위해, 그냥 pad sequence 이전것을 저장한다.  \n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_train_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(train_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_train_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(train_label, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_val_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(val_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_val_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(val_label, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_test_lst.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(test_input_lst, f,indent=\"\\t\")\n",
        "with open('/content/gdrive/My Drive/CNS_NLP/naver_m_test_label.json', 'w',encoding='utf-8') as f:\n",
        "  json.dump(test_label, f,indent=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5vNIjS_D3fX"
      },
      "source": [
        "**이후 과정은 별도 posting으로 운영한다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAJX3tuhD9rb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title:  \"Neuralnet \"\n",
    "excerpt: \"Basic Neural Net using numpy,tensor-flow,keras\"\n",
    "\n",
    "categories:\n",
    "  - Deep-Learning\n",
    "tags:\n",
    "  - DL\n",
    "  - Neuralnet Using Numpy\n",
    "  - 밑바닥부터 시작하는 딥러닝\n",
    "  - 딥러닝\n",
    "last_modified_at: 2020-02-16T08:06:00-05:00\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MNIST 데이터를 활용하여, 기본 뉴럴넷을 구성해보자\n",
    "> minst 데이터는 기본 keras에 있는 걸 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images,train_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape (60000, 28, 28)\n",
      "train_labels.shape (60000,)\n",
      "test_images.shape (10000, 28, 28)\n",
      "test_labels (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_images.shape\",train_images.shape)\n",
    "print(\"train_labels.shape\",train_labels.shape)\n",
    "print(\"test_images.shape\",test_images.shape)\n",
    "print(\"test_labels\",test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000,28*28)\n",
    "test_images = test_images.reshape(10000,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 본 실습은 \"밑바닥부터 시작하는 Deep Learing\" = \"Deep Learning From Scratch\"를 활용하여, DL 기초부분을 설명함  \n",
    "+ 여기서는 역전파에 대해서, 수치미분 과 계산그래프의 개념을 통해서, 설명하는데, 주로 \"계산그래프\" 방식을 통해서, 딥러닝의 역전파 핵심을 설명한다. 이 부분은 별도로 공부하거나, 문의주시길.\n",
    "> 수치미분 : Lim f(x+h)/f(x) 극한의 개념을 가져갈때, 애기한 미분값. 가장 일반적인 설명에 사용함  \n",
    "> 계산그래프 : Andrej Karpathy 블로그,  Fei-Fei Le (stanford 교수) 의 아이디어임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TwoLayerNet class 를 만들어서, 진행함\n",
    "+ 내부에서 활용하기 위해 필요한 class 와 def 정의함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relu Class**\n",
    "> Activation 함수로 Relu를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affine Class**\n",
    "> WX+B 선현결합과 activation 함수의 결합한 1개의 Node를 말한다.\n",
    "> 여기서 사용된 코드들은 계산그래프 내용과 닿아있으며, 여기서는 설명을 생략한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W ## 해당 Node의 가중치 값을 자체 클래스 내부에 저장하기 위한것 - forward\n",
    "        self.b = b ## 해당 노드의 bias 값을 자체 클래스 내부에 저장하기 위한것  - forward\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None ## 역전파시 사용되는 미분값을 저장하기 위함. Node별 가중치 값이 1번 계산되고 저장되기에, 이 값만 알면, 100,1000,10만 번이든 무관하게 간단한 연산만으로 속도를 향상시킨다\n",
    "        self.db = None ## 이 점이 바로 \"오차역전파법\" 의 내용과 연관되어있다.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x ## 나중에 역전파일때, 사용하기 위해 노드의 변수로 저장해둔다. - backward\n",
    "        out = np.dot(self.x, self.W) + self.b ## 선형결합 결과값을 넘김 이 다음에 activation 함수가 이 결과값을 인자로 받는다.\n",
    "                                              ## 다른 책에서는 선형결합+activation 을 한개로 묶어서 설명하기도 한다.\n",
    "        return out\n",
    "    \n",
    "    ## network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "    ## affine layer는 2개임. 1layer:50,2layer:10\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) ## dout는 최종노드에서부터 오는 값. \n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **SoftmaxWithLoss Class**\n",
    "> 마지막 layer 로서, 소프트맥스 함수와 Loss 함수인 Cross Entropy 노드를 담고 있다.\n",
    "> 통상 Train 단계가 아닌 추론 단계에서는 Sofrmax With Loss 노드를 생략하기도 한다.\n",
    ">> 왜냐하면, ohe-hot 인코딩인드 아니든 y_pred 값이 (10 class 분류문제일 경우) 큰 값만을 가져서와 계산을 해도 문제가 없기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size ## 해석적 미분을 하면, 소프트맥스함수와 크로스엔트로피 결합은 (self.y - self.t) 와 같다.\n",
    "        else:\n",
    "            dx = self.y.copy()             ##일단 softmax 값으로 나온 예측값 y를 copy하고, y의 shape는 (batch_size,10) <minist class가 10개고, X 데이터 shape[1] 이 10 one-hot이기때문\n",
    "            dx[np.arange(batch_size), self.t] -= 1 ## 해당 예측값을 찾아서 1을 빼줌으로서  (self.y - self.t) 와 같게만듬. -1 에서 1은 정답레이블을 그냥 원-핫 취급하기 위해 빼준는것\n",
    "            dx = dx / batch_size            ## 예측값이 원-핫 형태이니, y-t 에서 t 도 원-핫 처럼 다뤄주기 위해서임\n",
    "        return dx\n",
    "            \n",
    "    def cross_entropy_error(self,y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "\n",
    "        # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x) # 오버플로 대책\n",
    "        return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        np.random.seed(42) ## 확인용\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size,)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        ##갱신여부 알고리즘 start check##\n",
    "        aa = self.params['W1'].copy()\n",
    "        bb = self.params['b1'].copy()\n",
    "        cc = self.params['W2'].copy()\n",
    "        dd = self.params['b2'].copy()\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "#         self.layers['Affine1'] = Affine(aa, bb)\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "#         self.layers['Affine2'] = Affine(cc, dd)\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        print(\"create TwoLayerNet\")\n",
    "        \n",
    "        ##갱신여부 알고리즘 End check##\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "#         print(\"loss function go\")\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "#     def numerical_gradient(self, x, t):\n",
    "#         loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "#         grads = {}\n",
    "#         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "#         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "#         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "#         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "#         return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "#         print(\"Affine1_dW\", np.sum(np.sum(self.layers[\"Affine1\"].dW)))\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "    def softmax(x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T\n",
    "        \n",
    "        x = x - np.max(x) # 오버플로 대책\n",
    "        return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 실제 TwoLayerNet 를 활용한 Mnist data 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images , test_images , train_labels , test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create TwoLayerNet\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_per_epoch 600.0\n",
      "0.10441666666666667 0.1028\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n",
      "0.11238333333333334 0.1136\n"
     ]
    }
   ],
   "source": [
    "iters_num = 6000\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "print(\"iter_per_epoch\",iter_per_epoch)\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "#     batch_mask = range(batch_size) 고정\n",
    "    x_batch = train_images[batch_mask]\n",
    "    t_batch = train_labels[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    ##갱신여부 start check##\n",
    "#     p1=network.params['W1'].sum()\n",
    "#     p2=network.params['b1'].sum()\n",
    "#     print(\"network_param\",p1,p2)\n",
    "#     tw1 = network.layers['Affine1'].W.sum()\n",
    "#     tb1 = network.layers['Affine1'].b.sum()\n",
    "#     print(\"before modify\",tw1,tb1)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "#     tw2 = network.layers['Affine1'].W.sum()\n",
    "#     tb2 = network.layers['Affine1'].b.sum()\n",
    "#     print(\"after modify\",tw2,tb2,\"\\n\")\n",
    "    ##갱신여부 end check##\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(train_images, train_labels)\n",
    "        test_acc = network.accuracy(test_images, test_labels)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loss_list),len(train_acc_list),len(test_acc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상기 정확도가 높지않은 이유는 몇가지 보정처리를 추가적으로 해줘야 하기 때문\n",
    "상세내용은 다음 기회에 다룬다.\n",
    "혹 W,b 의 갱신이 이루어지지 않는 것이 아닐까 추정했지만, 결론적으론 그건 아닌걸로."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중치, bias 갱신 부분 코드 체크\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]  \n",
    "        \n",
    "상기 부분은 TwoLayerNet 클래스의 params dict 객체안의 값을 갱신시킨다. 그러나, 실제로 갱신되어야 하는 값은, TwoLayerNet - layers(OrderDict 객체) - \"Affine1\",\"Relu\",\"Affine2\" - W,b 값들이다.  \n",
    "헌데, 확인해보니 가중치,bias갱신은 일어난다. 어찌된일일까?  \n",
    "\n",
    "비밀은 __call_by_reference__ 에 있다 (필자추정)  \n",
    "TwoLayerNet 에서, layers(OrderDict 객체) 의 value로 \"Affine calss\" 를 최초에 생성하는데, 이때의 구문은  \n",
    "##### \n",
    "    self.layers = OrderedDict()  \n",
    "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])  \n",
    "    self.layers['Relu1'] = Relu()  \n",
    "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])  \n",
    "    self.lastLayer = SoftmaxWithLoss()  \n",
    "여기서, **Affine(self.params['W1'], self.params['b1'])** 을 보면, 내부 TwoLayerNet 클래스의 params 변수로 생성했다. 이때, self.params['b1'] 와 같은 값들은, 메모리에 값(1,2같은)이 들어가는게 아니라  \n",
    "\"주소\" 값이 들어간다. 그렇게 때문에, self.params 의 값이 갱신되면, 그걸 바라보고 있는 Affine 클래스의 W,b (=\"Affine1\",\"Relu\",\"Affine2\" - W,b)들도 같이 변하게 되는 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

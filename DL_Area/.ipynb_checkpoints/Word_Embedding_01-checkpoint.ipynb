{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title:  \"Word Embedding using konlpy\"  \n",
    "excerpt: \"Text Analysis\"  \n",
    "\n",
    "categories:  \n",
    "  - Deep-Learning  \n",
    "tags:  \n",
    "  - Text Analysis\n",
    "  - embedding\n",
    "  - Colab\n",
    "last_modified_at: 2020-12-04T14:13:00-05:00\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference  \n",
    "* 사설 강의 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하기 내용은 본인 컴퓨터가 window os 다 보니, 전적으로 Collab 환경에서만 실행가능합니다.  \n",
    "(google colab 은 ubuntu라서 좀더 konlp가 쉽게 설치되는 것 같으니, 형태속 분석은 확실히 Colab에서~)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google 내 my drive 연동하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### konlpy라는 한국어 자연어처리 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy\n",
    "!pip install jpype1==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/content/KorQuAD_v1.0_train.json\") as f:\n",
    "  data = json.loads(f.read())\n",
    "\n",
    "PARAS = []\n",
    "for dat in data[\"data\"]:\n",
    "  for para in dat[\"paragraphs\"]:\n",
    "    PARAS.append(para[\"context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source Data :\n",
    "\n",
    "- 한국어 위키백과 단락 일부\n",
    "- KorQuAD 1.0 데이터 본문 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colab 에서, 기본적인 dataset은 content 으로 저장됩니다. 따라서 embedding 벡터로 만들기 위한 wiki 데이터를 content에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/content/KorQuAD_v1.0_train.json\") as f:\n",
    "  data = json.loads(f.read())\n",
    "\n",
    "PARAS = []\n",
    "for dat in data[\"data\"]:\n",
    "  for para in dat[\"paragraphs\"]:\n",
    "    PARAS.append(para[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"** Number of Paragraphs:\", len(PARAS))\n",
    "PARAS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran, Hannanum, Kkma, Okt\n",
    "komoran = Komoran()\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "okt = Okt()\n",
    "\n",
    "# 문장, 형태소분석기를 인풋으로 받아 쪼개진 문장을 리턴하는 함수 정의\n",
    "def tokenize(sentence, tokenizer):\n",
    "  return tokenizer.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"형태소 분석기마다 쪼개진 문장 특성이 조금씩 달라요\"\n",
    "print(\"한나눔:\", tokenize(sentence, hannanum))\n",
    "print(\"꼬꼬마:\", tokenize(sentence, kkma))\n",
    "print(\"코모란:\", tokenize(sentence, komoran))\n",
    "print(\"트위터:\", tokenize(sentence, okt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 활용한 komoran 분석기를 활용해서 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "  \"\"\" Your Code Here \"\"\"\n",
    "  return komoran.morphs(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어사전을 만들기 전, 학습용과 검증요 데이터로 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문단을 Train과 Validation으로 나누기\n",
    "import random\n",
    "random.seed(1)\n",
    "random.shuffle(PARAS)\n",
    "\n",
    "PARAS_tr = PARAS[:8000]\n",
    "PARAS_dev = PARAS[8000:]\n",
    "\n",
    "print(\"Train: {} | Val: {}\".format(len(PARAS_tr), len(PARAS_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step01. 토큰단위로 단어사전 만들기  \n",
    "> Counter 객체를 활용 빈도수가 높은 단어부터 추출될 수 있도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 셋에 있는 Paragraph를 형태소로 쪼개고,토큰의 빈도를 체크하기 \n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "vocab_freq = Counter()\n",
    "TOKEN_PARAS_tr = []\n",
    "for i,para in tqdm(enumerate(PARAS_tr)):\n",
    "    tokenized_para = tokenize(para) ## 함수 tokenize 에 의해 list 형태로 return 받는다.\n",
    "    for word in tokenized_para: ## return 받은 형태소마다, dict처럼 Counter 객체에 삽입\n",
    "        vocab_freq[word] += 1\n",
    "    \n",
    "    TOKEN_PARAS_tr.append(tokenized_para) ## 2차원. list 의 list 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step02.  단어의 Frequency에 따라 단어사전에 포함할 토큰 골라내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 70000\n",
    "most_common = vocab_freq.most_common(len(vocab_freq))\n",
    "\n",
    "print(\"가장 많이 나온 10개 토큰:\", most_common[:10])\n",
    "print(\"가장 적게 나온 10개 토큰:\",most_common[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 단어사전은 위에서 골라낸 토큰과 더불어 [PAD]와 [UNK] 토큰을 포함해야 합니다.\n",
    "\n",
    "딥러닝에서 [PAD]토큰은 0번 인덱스를 사용하는 것이 일반적입니다.\n",
    "\n",
    "따라서 0번째 자리에 [PAD], 1번째에 [UNK],\n",
    "2번째부터는 위에서 골라낸 vocabulary_set에 있는 단어들이 차례로 오는 vocabulary_list를 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 사전 생성을 위해 [PAD] , [UNK] 토큰을 포함하는 vocabulary_list 생성\n",
    "vocabulary_list = [\"[PAD]\", \"[UNK]\"]\n",
    "vocabulary_list.extend(vocabulary_set)\n",
    "\n",
    "print(\"최종 단어 개수: {}개\".format(len(vocabulary_list)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상입니다. 이후 Posting에서는 이를 이용하여, Embedding 계층 등을 공부하려 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('gpu_test': conda)",
   "language": "python",
   "name": "python37764bitgputestconda7b0bbf327036479ea1f0b0ead33b8789"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

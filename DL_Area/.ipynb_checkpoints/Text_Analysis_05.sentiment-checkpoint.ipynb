{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title:  \"Text Analysis Sentiment\"\n",
    "excerpt: \"Text Analysis\"  \n",
    "\n",
    "categories:  \n",
    "  - Deep-Learning  \n",
    "tags:  \n",
    "  - Text Analysis\n",
    "  - Opinion Review  \n",
    "  - Sentimental  \n",
    "last_modified_at: 2020-08-10T15:20:00-05:00\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference  \n",
    "* 파이썬 머신러닝 완벽가이드 - 권철민\n",
    "* NCIA shkim.hi@gmail.com  \n",
    "* [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/44249)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지도학습 기반 감성 분석 실습 – IMDB 영화평\n",
    "* [dataset](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kaggle 에서, download 가능  \n",
    "[kaggle 주소](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# review_df = pd.read_csv('D:/202007_JJH/AI-RNN/lab_data/ch01/dataset/labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)\n",
    "review_df = pd.read_csv('D:/★2020_ML_DL_Project/Alchemy/dataset/labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)\n",
    "review_df.head(3)\n",
    "## header = 0 : 0번째 row 를 header 로 쓰겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 사전 처리 - html 태그 제거 및 숫자/문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# <br> html 태그는 replace 함수로 공백으로 변환\n",
    "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
    "\n",
    "# 파이썬의 정규 표현식 모듈인 re를 이용하여 영어 문자열이 아닌 문자는 모두 공백으로 변환 \n",
    "review_df['review'] = review_df['review'].apply( lambda x : re.sub(\"[^a-zA-Z]\", \" \", x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay   Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him   The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music   Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene   Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
     ]
    }
   ],
   "source": [
    "# 영문 Text만 남기고 나머지는 제거 한다.\n",
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습/테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df.drop(['id','sentiment'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.3, random_state=156)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_df : class = label  \n",
    "feature_df : 영어문단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review\n",
      "0   With all this stuff going down at the moment ...\n",
      "1     The Classic War of the Worlds   by Timothy ...\n",
      "2   The film starts with a manager  Nicholas Bell...\n",
      "3   It must be assumed that those who praised thi...\n",
      "4   Superbly trashy and wondrously unpretentious ...\n"
     ]
    }
   ],
   "source": [
    "print(feature_df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1792\n",
      "2469\n",
      "598\n",
      "1871\n",
      "2032\n",
      "1061\n",
      "733\n",
      "1172\n",
      "713\n",
      "592\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(X_train.review.values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 문장들의 길이가 모두 다르다는 것을 알 수 있따."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pipeline을 통해 Count기반 피처 벡터화 및 머신러닝 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스톱 워드는 English, filtering, ngram은 (1,2)로 설정해 CountVectorization수행. \n",
    "# LogisticRegression의 C는 10으로 설정. \n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words='english', ngram_range=(1,2) )),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8860, ROC-AUC는 0.9503\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc때문에 수행.  \n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test ,pred),roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99999759e-01 2.40712242e-07]\n",
      " [9.60692236e-01 3.93077644e-02]\n",
      " [9.85290870e-01 1.47091302e-02]\n",
      " [6.42742280e-01 3.57257720e-01]\n",
      " [9.69684380e-01 3.03156197e-02]]\n"
     ]
    }
   ],
   "source": [
    "# for test by SOO\n",
    "t = pipeline.predict_proba(X_test['review'])\n",
    "print(t[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.40712242e-07 3.93077644e-02 1.47091302e-02 3.57257720e-01\n",
      " 3.03156197e-02]\n"
     ]
    }
   ],
   "source": [
    "# for test by SOO\n",
    "t = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "print(t[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비지도학습 기반 감성 분석 - 감성 어휘 사전을 이용한 분석\n",
    "* 비지도 감성 분석은 Lexicon(어휘집)을 기반으로 하는 것임\n",
    "* Lexicon은 일반적으로 어휘집을 의미하지만 여기서는 주로 감성만을 분석하기 위해 지원하는 감성 어휘 사전임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiWordNet을 이용한 Sentiment Analysis \n",
    "* WordNet Synset과 SentiWordNet SentiSynset 클래스의 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* WordNet(워드넷)은 자연어 처리(NLP, Natural Language Processing)를 위한 특화된 사전이라고 볼 수 있다.\n",
    "* WordNet(워드넷)은 영어의 의미 어휘목록이다. WordNet은 영어 단어를 'synset'이라는 유의어 집단(동의어 집합)으로 분류하여 간략하고 일반적인 정의를 제공하고, 이러한 어휘목록 사이의 다양한 의미 관계를 기록한다.\n",
    "* WordNet은 다양한 상황에서 같은 어휘라도 다르게 사용되는 어휘의 semantic 정보를 제공하며, 이를 위해 각각의 품사(명사, 동사, 형용사, 부사)로 구성된 개별 단어를 Synset이라는 개념을 이용해 표현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn ## 동의사전, 유의사전\n",
    "\n",
    "term = 'present'\n",
    "\n",
    "# 'present'라는 단어로 wordnet의 synsets 생성. \n",
    "synsets = wn.synsets(term)\n",
    "print('synsets() 반환 type :', type(synsets))\n",
    "print('synsets() 반환 값 갯수:', len(synsets))\n",
    "print('synsets() 반환 값 :', synsets)\n",
    "print(synsets[0].name())\n",
    "print(synsets[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for synset in synsets :\n",
    "    print('##### Synset name : ', synset.name(),'#####')\n",
    "    print('POS :',synset.lexname()) ## 문법적인 이름이 뭔지를 보여준다.\n",
    "    print('Definition:',synset.definition())\n",
    "    print('Lemmas:',synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synset 객체를 단어별로 생성합니다. \n",
    "tree = wn.synset('tree.n.01') ## Synset 객체만들기\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog = wn.synset('dog.n.01')\n",
    "entities = [tree , lion , tiger , cat , dog]\n",
    "\n",
    "entity_names = [ entity.name().split('.')[0] for entity in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[0].name().split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tree.path_similarity(lion), lion.path_similarity(tiger), dog.path_similarity(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 synset 들을 iteration 하면서 다른 단어들의 synset과 유사도를 측정합니다. \n",
    "similarities = []\n",
    "for entity in entities:\n",
    "    similarity = [ round(entity.path_similarity(compared_entity), 2)  for compared_entity in entities ]\n",
    "    similarities.append(similarity)\n",
    "    \n",
    "# 개별 단어별 synset과 다른 단어의 synset과의 유사도를 DataFrame형태로 저장합니다.  \n",
    "similarity_df = pd.DataFrame(similarities , columns=entity_names,index=entity_names)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "senti_synsets = list(swn.senti_synsets('slow'))\n",
    "print('senti_synsets() 반환 type :', type(senti_synsets))\n",
    "print('senti_synsets() 반환 값 갯수:', len(senti_synsets))\n",
    "print('senti_synsets() 반환 값 :', senti_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "father = swn.senti_synset('father.n.01')\n",
    "print('father 긍정감성 지수: ', father.pos_score())\n",
    "print('father 부정감성 지수: ', father.neg_score())\n",
    "print('father 객관성 지수: ', father.obj_score())\n",
    "print('\\n')\n",
    "fabulous = swn.senti_synset('fabulous.a.01')\n",
    "print('fabulous 긍정감성 지수: ',fabulous .pos_score())\n",
    "print('fabulous 부정감성 지수: ',fabulous .neg_score())\n",
    "print('fabulous 객관성 지수: ',fabulous .obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "## PennTreebank 은 또다른 사전. NLTK와는 다른 또다른 사전이다.\n",
    "# 간단한 NTLK PennTreebank Tag를 기반으로 WordNet기반의 품사 Tag로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "def swn_polarity(text):\n",
    "    # 감성 지수 초기화 \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    # 분해된 문장별로 단어 토큰 -> 품사 태깅 후에 SentiSynset 생성 -> 감성 지수 합산 \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # NTLK 기반의 품사 태깅 문장 추출  \n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        for word , tag in tagged_sentence:\n",
    "            \n",
    "            # WordNet 기반 품사 태깅과 어근 추출\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN , wn.ADJ, wn.ADV): # 동사같은 품사는 건너뛴다는 의미, 주어도 건너뛴다.\n",
    "                continue                   \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma: ## 알수없는 단어라면, 버린다.\n",
    "                continue\n",
    "            # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체를 생성. \n",
    "            synsets = wn.synsets(lemma , pos=wn_tag) ## 이해가능한 단어들만, 유의어를 찾는다.\n",
    "            if not synsets: # 근데 유의어가 없으면 또 버리고\n",
    "                continue\n",
    "            # sentiwordnet의 감성 단어 분석으로 감성 synset 추출\n",
    "            # 모든 단어에 대해 긍정 감성 지수는 +로 부정 감성 지수는 -로 합산해 감성 지수 계산. \n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())           \n",
    "            tokens_count += 1\n",
    "    \n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    \n",
    "    # 총 score가 0 이상일 경우 긍정(Positive) 1, 그렇지 않을 경우 부정(Negative) 0 반환\n",
    "    if sentiment >= 0 :\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['preds'] = review_df['review'].apply( lambda x : swn_polarity(x) )\n",
    "y_target = review_df['sentiment'].values\n",
    "preds = review_df['preds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_target[:10])\n",
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score \n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "\n",
    "print(confusion_matrix( y_target, preds))\n",
    "print(\"정확도:\", accuracy_score(y_target , preds))\n",
    "print(\"정밀도:\", precision_score(y_target , preds))\n",
    "print(\"재현율:\", recall_score(y_target, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER lexicon을 이용한 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "senti_scores = senti_analyzer.polarity_scores(review_df['review'][0])\n",
    "print(senti_scores)  # compound : neg/neu/pos를 조합한 값 -1 ~ 1, 보통 0.1이상이면 긍정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader라고 하는 것들은 sentiwordnet 과는 다른 지수를 제공해준다. 이정도로 이해하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_polarity(review,threshold=0.1):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    \n",
    "    # compound 값에 기반하여 threshold 입력값보다 크면 1, 그렇지 않으면 0을 반환 \n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 1 if agg_score >= threshold else 0\n",
    "    return final_sentiment\n",
    "\n",
    "# apply lambda 식을 이용하여 레코드별로 vader_polarity( )를 수행하고 결과를 'vader_preds'에 저장\n",
    "review_df['vader_preds'] = review_df['review'].apply( lambda x : vader_polarity(x, 0.1) )\n",
    "y_target = review_df['sentiment'].values\n",
    "vader_preds = review_df['vader_preds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#### VADER 예측 성능 평가 ####')\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score \n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "\n",
    "print(confusion_matrix( y_target, vader_preds))\n",
    "print(\"정확도:\", accuracy_score(y_target , vader_preds))\n",
    "print(\"정밀도:\", precision_score(y_target , vader_preds))\n",
    "print(\"재현율:\", recall_score(y_target, vader_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
